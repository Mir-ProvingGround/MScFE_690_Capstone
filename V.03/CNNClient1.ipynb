{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ef361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waiting for prediction order\n",
    "import socket\n",
    "\n",
    "# Create a socket object\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Get local machine name\n",
    "host = socket.gethostname()\n",
    "port = 8999\n",
    "msg='none'\n",
    "# Bind to the port\n",
    "server_socket.bind((host, port))\n",
    "\n",
    "server_socket.listen(5)\n",
    "print(\"Standby for prediction order\")\n",
    "while True:\n",
    "    # Establish a connection\n",
    "    client_socket, addr = server_socket.accept()\n",
    "    print(\"Receive prediction order\")\n",
    "    # Receive no more than 1024 bytes\n",
    "    msg = (client_socket.recv(1024)).decode('utf-8')\n",
    "    client_socket.close()\n",
    "    break\n",
    "\n",
    "if(msg=='MSFT'):\n",
    "    #Time recording\n",
    "    import datetime\n",
    "    startCNN = datetime.datetime.now()\n",
    "\n",
    "    #Library list\n",
    "    # Library imports\n",
    "    import keras\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "    from keras import optimizers, regularizers\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import yfinance as yf\n",
    "    from sklearn import preprocessing\n",
    "    import matplotlib.pyplot as plt\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    import ta\n",
    "    #Set random parameter\n",
    "    np.random.seed(4)\n",
    "    tf.random.set_seed(4)\n",
    "\n",
    "    # Get the data for the stock MSFT between '2012-02-01' to '2019-03-05'\n",
    "    df = yf.download('MSFT','2012-02-01','2019-03-05')\n",
    "    # Heiken Ashi calculation\n",
    "    heiken_ashi_df = df.copy()\n",
    "    heiken_ashi_df['HA_Close'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n",
    "    heiken_ashi_df['HA_Open'] = (df['Open'] + df['Close']) / 2\n",
    "    heiken_ashi_df['HA_High'] = df[['Open', 'Close', 'High']].max(axis=1)\n",
    "    heiken_ashi_df['HA_Low'] = df[['Open', 'Close', 'Low']].min(axis=1)\n",
    "\n",
    "    # Additional technical indicators\n",
    "    df['RSI'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "    df['MACD'] = ta.trend.macd_diff(df['Close'])\n",
    "    df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "\n",
    "    # Combine all features into one DataFrame\n",
    "    df = df.join(heiken_ashi_df[['HA_Open', 'HA_High', 'HA_Low', 'HA_Close']])\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    next_day_closed = df['Close'].shift(-1, axis=0)\n",
    "    next_day_closed.drop(index=next_day_closed.index[len(next_day_closed)-1], axis=0, inplace=True)\n",
    "    df.drop(index=df.index[len(df)-1], axis=0, inplace=True)\n",
    "    date = df.index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.tail()\n",
    "    open_values=df['Open'].to_numpy()\n",
    "    closed_values=df['Close'].to_numpy()\n",
    "    date_values=date.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    #convert as an array will be used as feature in classifier\n",
    "    data = df.to_numpy()\n",
    "    #convert next closed data to array, will be used as label\n",
    "    next_day_closed_values=next_day_closed.to_numpy()\n",
    "    # dataset split and number of rows (history) involved in prediction\n",
    "    test_split = 0.5\n",
    "    history_points=100\n",
    "\n",
    "    #normalized data\n",
    "    data_normaliser = preprocessing.MinMaxScaler()\n",
    "    #data=data.reshape(-1, 1)\n",
    "    data_normalised = data_normaliser.fit_transform(data)\n",
    "    next_day_closed_values = next_day_closed_values.reshape(-1, 1)\n",
    "    next_day_closed_values_normalised = data_normaliser.fit_transform(next_day_closed_values)\n",
    "\n",
    "    data_histories_normalised = np.array([data_normalised[i:i + history_points].copy() for i in range(len(data_normalised) - history_points)])\n",
    "    n = 1558#int(data_histories_normalised.shape[0] * test_split)\n",
    "    next_day_closed_values_normalised = np.array([next_day_closed_values_normalised[i + history_points].copy() for i in range(len(next_day_closed_values_normalised) - history_points)])\n",
    "    next_day_closed_values = np.array([next_day_closed_values[i + history_points].copy() for i in range(len(next_day_closed_values) - history_points)])\n",
    "    open_values = np.array([open_values[i + history_points].copy() for i in range(len(open_values) - history_points)])\n",
    "    closed_values= np.array([closed_values[i + history_points].copy() for i in range(len(closed_values) - history_points)])\n",
    "    date_values= np.array([date_values[i + history_points].copy() for i in range(len(date_values) - history_points)])\n",
    "\n",
    "    # Generate 12x1 images from the rolling windows\n",
    "    def create_images(data):\n",
    "        images = []\n",
    "        for i in range(len(data)):\n",
    "            window = data[i]\n",
    "            image = window.T\n",
    "            #print(image.shape)\n",
    "            if image.shape == (13, 100):\n",
    "                images.append(image)\n",
    "        return np.array(images)\n",
    "    #\n",
    "\n",
    "    images = create_images(data_histories_normalised)\n",
    "    #print(len(images))\n",
    "    #TRAIN DATA\n",
    "    data_train = images[:n]#data_histories_normalised[:n]\n",
    "    y_train = next_day_closed_values_normalised[:n]\n",
    "\n",
    "    #TEST DATA\n",
    "    data_test = images[n:]#data_histories_normalised[n:]\n",
    "    y_test = next_day_closed_values_normalised[n:]\n",
    "    #print(len(y_test))\n",
    "    #print(len(data_test))\n",
    "    # Reshape for CNN\n",
    "    data_train = data_train.reshape(-1, 13, 1, 1)\n",
    "    data_test = data_test.reshape(-1, 13, 1, 1)\n",
    "\n",
    "    unscaled_y_test = next_day_closed_values[n:]\n",
    "    unscaled_y=next_day_closed_values\n",
    "    stacking = pd.DataFrame(columns=['date_time','open', 'close','delta_next_day', 'nextclose'])\n",
    "    stacking['date_time'] = date_values\n",
    "    stacking['open'] = open_values\n",
    "    stacking['close'] = closed_values\n",
    "    stacking['nextclose'] = next_day_closed\n",
    "\n",
    "\n",
    "    lr=0.0001\n",
    "    #CNN MODEL\n",
    "    mse_=0\n",
    "    nb_prediction=50\n",
    "    for ii in range(nb_prediction):\n",
    "        # Define CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), padding='same', input_shape=(100, 13, 1), kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(negative_slope=0.1))  # Use negative_slope instead of alpha\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(negative_slope=0.1))  # Use negative_slope instead of alpha\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Conv2D(256, kernel_size=(2, 2), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(LeakyReLU(negative_slope=0.1))  # Use negative_slope instead of alpha\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.35))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        # Compile the model\n",
    "        nadam = optimizers.Nadam(learning_rate=ii*lr+lr)\n",
    "        model.compile(optimizer=nadam, loss='mse')\n",
    "\n",
    "        # Add early stopping and learning rate scheduler\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)\n",
    "\n",
    "        # Train the model\n",
    "        data_train=tf.reshape(data_train,(1558,100,13,1))\n",
    "        model.fit(data_train, y_train, batch_size=64, epochs=25)\n",
    "\n",
    "        # Test the model\n",
    "        data_test=tf.reshape(data_test,(90,100,13,1))\n",
    "        images=tf.reshape(images,(1648,100,13,1))\n",
    "        # evaluation\n",
    "        y_test_predicted = model.predict(data_test)\n",
    "        y_test_predicted = data_normaliser.inverse_transform(y_test_predicted)\n",
    "        y_predicted = model.predict(images)#data_histories_normalised)\n",
    "        y_predicted = data_normaliser.inverse_transform(y_predicted)\n",
    "        unscaled_y_test = np.reshape(unscaled_y_test, (-1, 1))\n",
    "     #   print(unscaled_y_test.shape)\n",
    "     #   print(y_test_predicted.shape)\n",
    "\n",
    "        assert unscaled_y_test.shape == y_test_predicted.shape\n",
    "        #real_mse = np.mean(np.square(unscaled_y_test - y_test_predicted))\n",
    "        #mse=real_mse / (np.max(unscaled_y_test) - np.min(unscaled_y_test)) * 100\n",
    "        #scaled_mse = scaled_mse+real_mse / (np.max(unscaled_y_test) - np.min(unscaled_y_test)) * 100\n",
    "        #print(\"Prediction: \",ii)\n",
    "        #print('MSE:',tf.keras.losses.MSE(tf.expand_dims(srcTF, axis=-1) , tf.expand_dims(tgtTF, axis=-1)) )\n",
    "\n",
    "        plt.gcf().set_size_inches(22, 15, forward=True)\n",
    "        start = 0\n",
    "        end = -1\n",
    "        #real = plt.plot(unscaled_y_test[start:end], label='real')\n",
    "        #pred = plt.plot(y_test_predicted[start:end], label='predicted')\n",
    "        #real = plt.plot(unscaled_y[start:end], label='real')\n",
    "        #pred = plt.plot(y_predicted[start:end], label='predicted')\n",
    "        #plt.legend(['Real', 'Predicted'])\n",
    "        #plt.show()\n",
    "        #from datetime import datetime\n",
    "        #model.save(f'basic_model.h5')\n",
    "\n",
    "        #If next_day_closed_price is higher than today closed price for given threshold, then long or buy\n",
    "        #If the difference is negative, short or sell, otherwise hold.\n",
    "        thresh = 0.1\n",
    "        hold_=0\n",
    "        long_=1\n",
    "        short_=2\n",
    "        strategy=[]\n",
    "        closed_price_today=[]\n",
    "        for i in range (len(unscaled_y)):\n",
    "            closed_price_today_=unscaled_y[i]\n",
    "            closed_price_today.append(closed_price_today_)\n",
    "            predicted_closed_price_tomorrow = y_predicted[i]\n",
    "            delta=predicted_closed_price_tomorrow-closed_price_today_\n",
    "            if delta > thresh:\n",
    "                strategy.append(long_)\n",
    "            elif delta < 0:\n",
    "                strategy.append(short_)\n",
    "            else:\n",
    "                strategy.append(hold_)\n",
    "        mse=mean_squared_error(next_day_closed_values, y_predicted)\n",
    "        mse_=mse_+mse\n",
    "        pt = datetime.datetime.now()-startCNN\n",
    "        total_seconds = pt.total_seconds()\n",
    "        print('Step 1. Number_of_prediction: ',(ii+1),' Average_MSE: ',mse_/(ii+1),' Execution_time (s): ',total_seconds,\" Learning_rate:\", ii*lr+lr)\n",
    "        S='prediction_'+str(ii)\n",
    "        stacking[S] = strategy\n",
    "        print(datetime.datetime.now()-startCNN)\n",
    "\n",
    "    stacking['delta_next_day'] = next_day_closed_values-y_predicted\n",
    "    stacking['nextclose']=y_predicted#stacking.drop['nextclose'], axis=1, inplace=True)\n",
    "    stacking.to_csv('CNNs.csv')\n",
    "\n",
    "    today = plt.plot(next_day_closed_values, label='Tomorrow')\n",
    "    tomorrow = plt.plot(y_predicted, label='Tomorrow_Predicted')\n",
    "    plt.legend(['Tomorrow', 'Tomorrow_Predicted_CNNs'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Create a socket object\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = socket.gethostname()  # Get local machine name\n",
    "port = 12345  # Reserve a port for your service\n",
    "\n",
    "client_socket.connect((host, port))\n",
    "\n",
    "with open('distCNNs1.csv', 'rb') as f:\n",
    "    print(\"Sending file...\")\n",
    "    while True:\n",
    "        data = f.read(1024)\n",
    "        if not data:\n",
    "            break\n",
    "        client_socket.send(data)\n",
    "    print(\"File sent successfully\")\n",
    "\n",
    "client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d462b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
