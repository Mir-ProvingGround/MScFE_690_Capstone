{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening...\n"
     ]
    }
   ],
   "source": [
    "#Sending prediction order to client\n",
    "import datetime\n",
    "import socket\n",
    "startCNN = datetime.datetime.now()\n",
    "\n",
    "# Create a socket object for client1\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# One connection from local machine\n",
    "host = socket.gethostname()\n",
    "port = 9999\n",
    "\n",
    "# Connection to hostname on the port\n",
    "client_socket.connect((host, port))\n",
    "\n",
    "#send ticker and other prediction properties if necessary\n",
    "client_socket.send( bytes('MSFT', 'utf-8'))\n",
    "client_socket.close()\n",
    "\n",
    "# Create a socket object for client2\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# One connection from local machine\n",
    "host = \"192.168.15.198\"#socket.gethostname()\n",
    "port = 8999\n",
    "\n",
    "# Connection to hostname on the port\n",
    "client_socket.connect((host, port))\n",
    "\n",
    "#send ticker and other prediction properties if necessary\n",
    "client_socket.send( bytes('MSFT', 'utf-8'))\n",
    "client_socket.close()\n",
    "\n",
    "#Waiting for prediction file\n",
    "# Create another socket object\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "host = socket.gethostname()  # Get local machine name\n",
    "port = 12345  # Reserve a port for your service\n",
    "server_socket.bind((host, port))  # Bind to the port\n",
    "server_socket.listen(5)  # Now wait for client connection\n",
    "\n",
    "print(\"Server is listening...\")\n",
    "i=0\n",
    "filename=''\n",
    "while True:\n",
    "    client_socket, addr = server_socket.accept()  # Establish connection with client\n",
    "    print(f\"Got connection from {addr}\")\n",
    "    if(i==0):\n",
    "        filename='pred0.csv'\n",
    "    if(i==1):\n",
    "        filename='pred1.csv'\n",
    "    with open(filename, 'wb') as f:\n",
    "        print(\"Receiving file... \", i)\n",
    "        while True:\n",
    "            data = client_socket.recv(1024)\n",
    "            if not data:\n",
    "                break\n",
    "            f.write(data)\n",
    "        print(\"File received successfully\")\n",
    "    i=i+1\n",
    "    client_socket.close()\n",
    "    if(i==2):\n",
    "        break\n",
    "pt = datetime.datetime.now()-startCNN\n",
    "total_seconds = pt.total_seconds()\n",
    "print(\"Step-1 complete transmission in \",total_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "distCNN = pd.read_csv('distCNNs.csv')\n",
    "distCNN1 = pd.read_csv('distCNNs1.csv')\n",
    "distCNN1 = distCNN1.drop(['date_time','open','close','delta_next_day','nextclose'], axis=1)\n",
    "distCNNs = pd.concat([distCNN, distCNN1], axis=1)\n",
    "distCNNs.to_csv('distCNNs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4nK23SHEU3M"
   },
   "source": [
    "STEP 2 RL LEARNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split\n",
    "import pandas as pd\n",
    "df_ = pd.read_csv('distCNNs.csv')\n",
    "df_1=df_.head(1558)\n",
    "n=len(df_)-1558\n",
    "df_2=df_[-n:]\n",
    "df_1.to_csv('distCNNs_train.csv')\n",
    "df_2.to_csv('distCNNs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input for RL agent\n",
    "#in1 number of actions ---in1\n",
    "#in2: probability of performing explorations ---in2\n",
    "#in3: initializer ---in3\n",
    "#in4: folder name where experiments results will be written ---in4\n",
    "#in5: optimizer ---in5  \n",
    "#example: 3 0.3 relu teste-adam-0.3-relu  adam  \n",
    "in1='3'\n",
    "in2='0.5'\n",
    "in3='tanha'\n",
    "in4='distCNNs'\n",
    "in5='adadelta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold 0..long 1..short 2\n",
    "#Calculating the metrics\n",
    "from rl.callbacks import Callback\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self):\n",
    "        #Initially, the metrics are zero\n",
    "        self.episodes = 0\n",
    "        self.rewardSum = 0\n",
    "        self.accuracy = 0\n",
    "        self.coverage = 0\n",
    "        self.short = 0\n",
    "        self.long = 0\n",
    "        self.shortAcc =0\n",
    "        self.longAcc =0\n",
    "        self.longPrec =0\n",
    "        self.shortPrec =0\n",
    "        self.marketRise =0\n",
    "        self.marketFall =0\n",
    "    def reset(self):\n",
    "        self.episodes = 0\n",
    "        self.rewardSum = 0\n",
    "        self.accuracy = 0\n",
    "        self.coverage = 0\n",
    "        self.short = 0\n",
    "        self.long = 0\n",
    "        self.shortAcc =0\n",
    "        self.longAcc =0\n",
    "        self.longPrec =0\n",
    "        self.shortPrec =0\n",
    "        self.marketRise =0\n",
    "        self.marketFall =0\n",
    "    def on_episode_end(self, action, reward, market):\n",
    "        self.episodes+=1\n",
    "        self.rewardSum+=reward\n",
    "        self.coverage+=1 if (action != 0) else 0\n",
    "        self.accuracy+=1 if (reward >= 0 and action != 0) else 0\n",
    "        self.short +=1 if(action == 2) else 0\n",
    "        self.long +=1 if(action == 1) else 0\n",
    "        self.shortAcc +=1 if(action == 2 and reward >=0) else 0\n",
    "        self.longAcc +=1 if(action == 1 and reward >=0) else 0\n",
    "        if(market>0):\n",
    "            self.marketRise+=1\n",
    "            self.longPrec+=1 if(action == 1) else 0\n",
    "        elif(market<0):\n",
    "            self.marketFall+=1\n",
    "            self.shortPrec+=1 if(action == 2) else 0\n",
    "\n",
    "    def getInfo(self):\n",
    "        acc = 0\n",
    "        cov = 0\n",
    "        short = 0\n",
    "        long = 0\n",
    "        longAcc = 0\n",
    "        shortAcc = 0\n",
    "        longPrec = 0\n",
    "        shortPrec = 0\n",
    "        \n",
    "        if self.coverage > 0:\n",
    "            acc = self.accuracy/self.coverage\n",
    "        if self.episodes > 0:\n",
    "            cov = self.coverage/self.episodes\n",
    "            short = self.short/self.episodes\n",
    "            long = self.long/self.episodes\n",
    "        if self.short > 0:\n",
    "            shortAcc = self.shortAcc/self.short\n",
    "        if self.long > 0:\n",
    "            longAcc = self.longAcc/self.long\n",
    "        if self.marketRise > 0:\n",
    "            longPrec = self.longPrec/self.marketRise\n",
    "        if self.marketFall > 0:\n",
    "            shortPrec = self.shortPrec/self.marketFall\n",
    "        return self.episodes,cov,acc,self.rewardSum,long,short,longAcc,shortAcc,longPrec,shortPrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym for reinforcement learning\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "#import Callback\n",
    "\n",
    "class SpEnv1(gym.Env):\n",
    "    continuous = False\n",
    "    def __init__(self, data, callback = None, ensamble = None, columnName = \"iteration-1\"):\n",
    "        self.episode=1\n",
    "        self.data=data\n",
    "        self.output=False\n",
    "        #ensamble is the table of validation and testing\n",
    "        #If its none, you will not save csvs of validation and testing    \n",
    "        if(ensamble is not None): # managing the ensamble output (maybe in the wrong way)\n",
    "            self.output=True\n",
    "            self.ensamble=ensamble\n",
    "            self.columnName = columnName\n",
    "            self.ensamble[self.columnName]=0\n",
    "        self.low = numpy.array([-numpy.inf])\n",
    "        self.high = numpy.array([+numpy.inf])\n",
    "        self.action_space = gym.spaces.Box(low=numpy.array([0]),high= numpy.array([2]), dtype=int)\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=numpy.float32)\n",
    "        self.currentObservation = 0\n",
    "        self.done = False\n",
    "        self.limit = len(data)      \n",
    "        self.reward = None\n",
    "        self.possibleGain = 0\n",
    "        self.openValue = 0\n",
    "        self.closeValue = 0\n",
    "        self.callback=callback\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward=0\n",
    "        self.possibleGain = self.data.iloc[self.currentObservation]['delta_next_day']\n",
    "        self.possibleGain = self.data.iloc[self.currentObservation]['delta_next_day']\n",
    "        if(action == 1):\n",
    "            self.reward = self.possibleGain\n",
    "        elif(action==2):\n",
    "            self.reward = (-self.possibleGain)\n",
    "        elif(action==0):\n",
    "            self.reward = 0\n",
    "        self.done=True\n",
    "        if(self.callback!=None and self.done):\n",
    "            self.callback.on_episode_end(action,self.reward,self.possibleGain)\n",
    "        if(self.output):\n",
    "            self.ensamble.at[self.data.iloc[self.currentObservation]['date_time'],self.columnName]=action                   \n",
    "        self.episode+=1   \n",
    "        self.currentObservation+=1        \n",
    "        if(self.currentObservation>=self.limit):\n",
    "            self.currentObservation=0\n",
    "        return self.getObservation(), self.reward, self.done, {}\n",
    "\n",
    "    def reset(self): \n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.possibleGain = 0\n",
    "        return self.getObservation()\n",
    "\n",
    "    def getObservation(self):\n",
    "        predictionList = []\n",
    "        predictionList=numpy.array(self.data.iloc[self.currentObservation][\"prediction_26\":\"prediction_31\"])\n",
    "        return predictionList.ravel()\n",
    "    \n",
    "    def resetEnv(self):\n",
    "        self.currentObservation=0\n",
    "        self.episode=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the SPEnv library, which will perform the Agent actions themselves\n",
    "from Callback import ValidationCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import LeakyReLU, PReLU\n",
    "from keras.optimizers import *\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from keras_radam import RAdam\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy\n",
    "numpy.random.seed(0)\n",
    "\n",
    "class DeepQTrading1:    \n",
    "    #Class constructor\n",
    "    #model: Keras model considered\n",
    "    #explorations_iterations: a vector containing (i) probability of random predictions; (ii) how many iterations will be \n",
    "    #run by the algorithm (we run the algorithm several times-several iterations)  \n",
    "    #outputFile: name of the file to print metrics of the training\n",
    "    #ensembleFolderName: name of the file to print predictions\n",
    "    #optimizer: optimizer to run \n",
    "        \n",
    "    def __init__(self, model, nbActions, explorations_iterations, outputFile, ensembleFolderName, optimizer=\"adamax\"):\n",
    "        self.ensembleFolderName=ensembleFolderName\n",
    "        self.policy = EpsGreedyQPolicy()\n",
    "        self.explorations_iterations=explorations_iterations\n",
    "        self.nbActions=nbActions\n",
    "        self.model=model\n",
    "        #Define the memory\n",
    "        self.memory = SequentialMemory(limit=10000, window_length=1)\n",
    "        #Instantiate the agent with parameters received\n",
    "        self.agent = DQNAgent(model=self.model, policy=self.policy,  nb_actions=self.nbActions, memory=self.memory, nb_steps_warmup=200, target_model_update=1e-1, enable_double_dqn=True,enable_dueling_network=True)\n",
    "        #Compile the agent with the optimizer given as parameter\n",
    "        if optimizer==\"adamax\":        \n",
    "                self.agent.compile(Adamax(), metrics=['mae'])\n",
    "        if optimizer==\"adadelta\":        \n",
    "                self.agent.compile(Adadelta(), metrics=['mae'])\n",
    "        if optimizer==\"sgd\":        \n",
    "                self.agent.compile(SGD(), metrics=['mae'])\n",
    "        if optimizer==\"rmsprop\":        \n",
    "                self.agent.compile(RMSprop(), metrics=['mae'])\n",
    "        if optimizer==\"nadam\":        \n",
    "                self.agent.compile(Nadam(), metrics=['mae'])\n",
    "        if optimizer==\"adagrad\":        \n",
    "                self.agent.compile(Adagrad(), metrics=['mae'])\n",
    "        if optimizer==\"adam\":        \n",
    "                self.agent.compile(Adam(), metrics=['mae'])\n",
    "        if optimizer==\"radam\":        \n",
    "                self.agent.compile(RAdam(total_steps=5000, warmup_proportion=0.1, min_lr=1e-5), metrics=['mae'])\n",
    "\n",
    "        #Save the weights of the agents in the q.weights file\n",
    "        #Save random weights\n",
    "        self.agent.save_weights(\"q.weights\", overwrite=True)\n",
    "        #Load data\n",
    "        self.train_data= pd.read_csv('distCNNs_train.csv')#('./dataset/jpm/train_data2.csv')\n",
    "        self.validation_data=pd.read_csv('distCNNs.csv')#('./dataset/jpm/train_data2.csv')\n",
    "        self.test_data=pd.read_csv('distCNNs_test.csv')#('./dataset/jpm/test_data2.csv')\n",
    "                \n",
    "        #Call the callback for training, validation and test in order to show results for each iteration \n",
    "        self.trainer=ValidationCallback()\n",
    "        self.validator=ValidationCallback()\n",
    "        self.tester=ValidationCallback()\n",
    "        self.outputFileName=outputFile\n",
    "\n",
    "    def run(self):\n",
    "        #Initiates the environments, \n",
    "        trainEnv=validEnv=testEnv=\" \"\n",
    "         \n",
    "        if not os.path.exists(self.outputFileName):\n",
    "             os.makedirs(self.outputFileName)\n",
    "\n",
    "        file_name=self.outputFileName+\"/distCNNs-agent-training.csv\"\n",
    "        \n",
    "        self.outputFile=open(file_name, \"w+\")\n",
    "        #write the first row of the csv\n",
    "        self.outputFile.write(\n",
    "            \"Iteration,\"+\n",
    "            \"trainAccuracy,\"+\n",
    "            \"trainCoverage,\"+\n",
    "            \"trainReward,\"+\n",
    "            \"trainLong%,\"+\n",
    "            \"trainShort%,\"+\n",
    "            \"trainLongAcc,\"+\n",
    "            \"trainShortAcc,\"+\n",
    "            \"trainLongPrec,\"+\n",
    "            \"trainShortPrec,\"+\n",
    "\n",
    "            \"validationAccuracy,\"+\n",
    "            \"validationCoverage,\"+\n",
    "            \"validationReward,\"+\n",
    "            \"validationLong%,\"+\n",
    "            \"validationShort%,\"+\n",
    "            \"validationLongAcc,\"+\n",
    "            \"validationShortAcc,\"+\n",
    "            \"validLongPrec,\"+\n",
    "            \"validShortPrec,\"+\n",
    "                \n",
    "            \"testAccuracy,\"+\n",
    "            \"testCoverage,\"+\n",
    "            \"testReward,\"+\n",
    "            \"testLong%,\"+\n",
    "            \"testShort%,\"+\n",
    "            \"testLongAcc,\"+\n",
    "            \"testShortAcc,\"+\n",
    "            \"testLongPrec,\"+\n",
    "            \"testShortPrec\\n\")      \n",
    "        \n",
    "            \n",
    "        #Prepare the training and validation files for saving them later \n",
    "        ensambleValid=pd.DataFrame(index=self.validation_data[:].loc[:,'date_time'].drop_duplicates().tolist())\n",
    "        ensambleTest=pd.DataFrame(index=self.test_data[:].loc[:,'date_time'].drop_duplicates().tolist())\n",
    "            \n",
    "        #Put the name of the index for validation and testing\n",
    "        ensambleValid.index.name='date_time'\n",
    "        ensambleTest.index.name='date_time'\n",
    "            \n",
    "        #Explorations are epochs considered, or how many times the agent will play the game.  \n",
    "        for eps in self.explorations_iterations:\n",
    "\n",
    "            #policy will use eps[0] (explorations), so the randomness of predictions (actions) will happen with eps[0] of probability \n",
    "            self.policy.eps = eps[0]\n",
    "                \n",
    "            #there will be 25 iterations or eps[1] in explorations_iterations)\n",
    "            for i in range(0,eps[1]):\n",
    "                    \n",
    "                del(trainEnv)\n",
    "                #Define the training, validation and testing environments with their respective callbacks\n",
    "                trainEnv = SpEnv1(data=self.train_data, callback=self.trainer)\n",
    "                \n",
    "                del(validEnv)\n",
    "                validEnv=SpEnv1(data=self.validation_data,ensamble=ensambleValid,callback=self.validator,columnName=\"iteration\"+str(i))\n",
    "                \n",
    "                del(testEnv)  \n",
    "                testEnv=SpEnv1(data=self.test_data, callback=self.tester,ensamble=ensambleTest,columnName=\"iteration\"+str(i))\n",
    "\n",
    "                #Reset the callback\n",
    "                self.trainer.reset()\n",
    "                self.validator.reset()\n",
    "                self.tester.reset()\n",
    "\n",
    "                #Reset the training environment\n",
    "                trainEnv.resetEnv()\n",
    "                \n",
    "                #Train the agent\n",
    "                #The agent receives as input one environment\n",
    "                self.agent.fit(trainEnv,nb_steps=len(self.train_data),visualize=False,verbose=0)\n",
    "                \n",
    "                #Get the info from the train callback    \n",
    "                (_,trainCoverage,trainAccuracy,trainReward,trainLongPerc,                                              trainShortPerc,trainLongAcc,trainShortAcc,trainLongPrec,trainShortPrec)=self.trainer.getInfo()\n",
    "                \n",
    "                print(\"Iteration \" + str(i+1) + \" TRAIN:  accuracy: \" + str(trainAccuracy)+ \" coverage: \" + str(trainCoverage)+ \" reward: \" + str(trainReward))\n",
    "                             \n",
    "                #Reset the validation environment\n",
    "                validEnv.resetEnv()               \n",
    "                #Test the agent on validation data\n",
    "                self.agent.test(validEnv,nb_episodes=len(self.validation_data),visualize=False,verbose=0)\n",
    "                \n",
    "                #Get the info from the validation callback\n",
    "                (_,validCoverage,validAccuracy,validReward,validLongPerc,validShortPerc,\n",
    "validLongAcc,validShortAcc,validLongPrec,validShortPrec)=self.validator.getInfo()\n",
    "                #Print callback values on the screen\n",
    "                print(\"Iteration \" +str(i+1) + \" VALIDATION:  accuracy: \" + str(validAccuracy)+ \" coverage: \" + str(validCoverage)+ \" reward: \" + str(validReward))\n",
    "\n",
    "                #Reset the testing environment\n",
    "                testEnv.resetEnv()\n",
    "                #Test the agent on testing data\n",
    "                self.agent.test(testEnv,nb_episodes=len(self.test_data),visualize=False,verbose=0)\n",
    "                #Get the info from the testing callback\n",
    "                (_,testCoverage,testAccuracy,testReward,testLongPerc,testShortPerc,\n",
    "testLongAcc,testShortAcc,testLongPrec,testShortPrec)=self.tester.getInfo()\n",
    "                #Print callback values on the screen\n",
    "                print(\"Iteration \" +str(i+1) + \" TEST:  accuracy: \" + str(testAccuracy)+ \" coverage: \" + str(testCoverage)+ \" reward: \" + str(testReward))\n",
    "                print(\" \")\n",
    "                    \n",
    "                #write the metrics in a text file\n",
    "                self.outputFile.write(\n",
    "                    str(i)+\",\"+\n",
    "                    str(trainAccuracy)+\",\"+\n",
    "                    str(trainCoverage)+\",\"+\n",
    "                    str(trainReward)+\",\"+\n",
    "                    str(trainLongPerc)+\",\"+\n",
    "                    str(trainShortPerc)+\",\"+\n",
    "                    str(trainLongAcc)+\",\"+\n",
    "                    str(trainShortAcc)+\",\"+\n",
    "                    str(trainLongPrec)+\",\"+\n",
    "                    str(trainShortPrec)+\",\"+\n",
    "                       \n",
    "                    str(validAccuracy)+\",\"+\n",
    "                    str(validCoverage)+\",\"+\n",
    "                    str(validReward)+\",\"+\n",
    "                    str(validLongPerc)+\",\"+\n",
    "                    str(validShortPerc)+\",\"+\n",
    "                    str(validLongAcc)+\",\"+\n",
    "                    str(validShortAcc)+\",\"+\n",
    "                    str(validLongPrec)+\",\"+\n",
    "                    str(validShortPrec)+\",\"+\n",
    "                       \n",
    "                    str(testAccuracy)+\",\"+\n",
    "                    str(testCoverage)+\",\"+\n",
    "                    str(testReward)+\",\"+\n",
    "                    str(testLongPerc)+\",\"+\n",
    "                    str(testShortPerc)+\",\"+\n",
    "                    str(testLongAcc)+\",\"+\n",
    "                    str(testShortAcc)+\",\"+\n",
    "                    str(testLongPrec)+\",\"+\n",
    "                    str(testShortPrec)+\"\\n\")\n",
    "\n",
    "        #Close the file                \n",
    "        self.outputFile.close()\n",
    "\n",
    "        if not os.path.exists(\"./Output/ensemble/\"+self.ensembleFolderName):\n",
    "             os.makedirs(\"./Output/ensemble/\"+self.ensembleFolderName)\n",
    "\n",
    "        ensambleValid.to_csv(\"./Output/ensemble/\"+self.ensembleFolderName+\"/ensemble_valid.csv\")\n",
    "        ensambleTest.to_csv(\"./Output/ensemble/\"+self.ensembleFolderName+\"/ensemble_test.csv\")\n",
    "\n",
    "\n",
    "    #Function to end the Agent\n",
    "    def end(self):\n",
    "        print(\"FINISHED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startCNN = datetime.datetime.now()\n",
    "\"\"\"\"\"\n",
    "        python main.py <number_of_actions> <number_of_explorations> <activation> <output_file> <optimizer>        \n",
    "        ex: python3 main.py 3 0.3 selu teste-rmsprop-0.3-selu rmsprop\n",
    "        where:\n",
    "                <number_of_actions>: number of actions done by the agent. \n",
    "                <number_of_explorations>: in the RL training, this is the probability that the action taken is random or it obeys the Q-values found previously \n",
    "                <activation>: activation function of the double q-network layer we use as RL agent \n",
    "                <output_file>: where results will be written \n",
    "                <optimizer>: optimization approach of the RL network\n",
    "\n",
    "       Authors: Anselmo Ferreira, Alessandro Sebastian Podda and Andrea Corriga  \n",
    "\"\"\"\"\"\n",
    "\n",
    "#os library is used to define the GPU to be used by the code, needed only in cerain situations (Better not to use it, use only if the main gpu is Busy)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import LeakyReLU, PReLU, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "config =tf.compat.v1.ConfigProto\n",
    "tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "import random\n",
    "\n",
    "\n",
    "#There are three actions possible in the stock market\n",
    "#Hold(id 0): do nothing.\n",
    "#Long(id 1): It predicts that the stock market value will raise at the end of the day. \n",
    "#Short(id 2): It predicts that the stock market value will decrease at the end of the day.\n",
    "\n",
    "#NN composes one flatten layer to get 1000 dimensional vectors as input\n",
    "#One dense layer with 35 neurons with a given activation\n",
    "#One final Dense Layer with the number of actions considered and linear activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,29))) #10 number of prediction\n",
    "if(in3==\"relu\"):\n",
    "    model.add(Dense(35,activation='relu'))    \n",
    "if(in3==\"sigmoid\"):\n",
    "    model.add(Dense(35,activation='sigmoid'))    \n",
    "if(in3==\"linear\"):\n",
    "    model.add(Dense(35,activation='linear'))\n",
    "if(in3==\"tanh\"):\n",
    "    model.add(Dense(35,activation='tanh'))\n",
    "if(in3==\"selu\"):\n",
    "    model.add(Dense(35,activation='selu'))\n",
    "model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Dense(int(in1)))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "\n",
    "\n",
    "#Define the DeepQTrading class with the following parameters:\n",
    "dqt = DeepQTrading1(\n",
    "    model=model,\n",
    "    nbActions=int(in1),\n",
    "    explorations_iterations=[(round(float((in2))),25)],\n",
    "    outputFile=\"./Output/csv/\" + in4,\n",
    "    ensembleFolderName=in4,\n",
    "    optimizer=in5\n",
    "    )\n",
    "\n",
    "dqt.run()\n",
    "dqt.end()\n",
    "pt = datetime.datetime.now()-startCNN\n",
    "total_seconds = pt.total_seconds()\n",
    "print('Step 2. LR Learning stage execution_time (s): ',total_seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3 MAJORITY VOTING AND PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def majority_voting(df):\n",
    "    \n",
    "    local_df = df.copy()\n",
    "    x=local_df.loc[:,'iteration0':'iteration24']\n",
    "    local_df['ensemble']=x.mode(axis=1).iloc[:, 0]\n",
    "    local_df = local_df.drop(local_df.columns.difference(['ensemble']), axis=1)\n",
    "    return local_df\n",
    "    \n",
    "def ensemble(type, ensembleFolderName):\n",
    "\n",
    "    dollSum=0\n",
    "    rewSum=0\n",
    "    posSum=0\n",
    "    negSum=0\n",
    "    covSum=0 \n",
    "    numSum=0\n",
    "\n",
    "    values=[]\n",
    "    columns = [\"Experiment\",\"#Wins\",\"#Losses\",\"Dollars\",\"Coverage\",\"Accuracy\"]\n",
    "    \n",
    "    stacking=pd.read_csv(\"distCNNs_test.csv\",index_col='date_time')\n",
    "      \n",
    "    df=pd.read_csv(\"./Output/ensemble/distCNNs/ensemble_\"+type+\".csv\",index_col='date_time')\n",
    "        \n",
    "    df=majority_voting(df)\n",
    "      \n",
    "    num=0\n",
    "    rew=0\n",
    "    pos=0\n",
    "    neg=0\n",
    "    doll=0\n",
    "    cov=0\n",
    "\n",
    "  \n",
    "    #Lets iterate through each date and decision \n",
    "    for date, i in df.iterrows():\n",
    "     \n",
    "        #If the date in the predictions is in the index of stacking (which is also a date) \n",
    "        if date in stacking.index:\n",
    "             \n",
    "            num+=1\n",
    "              \n",
    "            #If the output is 1 (long)\n",
    "            if (i['ensemble']==1):\n",
    "                   \n",
    "                #If the close - open is positive at that day, we have earning money. Positives are equal to 1. Otherwise, no incrementation \n",
    "                pos+= 1 if (float(stacking.at[date,'delta_next_day'])) > 0 else 0\n",
    "\n",
    "                #If close - open is negative at that day, we are losing money. Negatives are equal to 1. Otherwise, no incrementation \n",
    "                neg+= 1 if (float(stacking.at[date,'delta_next_day'])) < 0 else 0\n",
    "\n",
    "                #Lets calculate the reward (positive or negative)\n",
    "                rew+=float(stacking.at[date,'delta_next_day'])\n",
    "                    \n",
    "                #In dollars, we just multiply by the stacking points by the differences \n",
    "                doll+=float(stacking.at[date,'delta_next_day'])\n",
    "\n",
    "                #There is coverage (of course) \n",
    "                cov+=1\n",
    "\n",
    "            #The same stuff happens for short.\n",
    "            elif (i['ensemble']==2):\n",
    "     \n",
    "                pos+= 1 if float(stacking.at[date,'delta_next_day']) < 0 else 0\n",
    "                neg+= 1 if float(stacking.at[date,'delta_next_day']) > 0 else 0\n",
    "                    \n",
    "                rew+=-float(stacking.at[date,'delta_next_day'])\n",
    "                cov+=1\n",
    "                doll+=-float(stacking.at[date,'delta_next_day'])\n",
    "                    \n",
    "\n",
    "\n",
    "    values.append([str(1),str(round(pos,2)),str(round(neg,2)),str(round(doll,2)),str(round(cov/num,2)),(str(round(pos/cov,2)) if (cov>0) else \"\")])\n",
    "        \n",
    "    #Now lets sum walk by walk \n",
    "    dollSum+=doll\n",
    "    rewSum+=rew\n",
    "    posSum+=pos\n",
    "    negSum+=neg\n",
    "    covSum+=cov\n",
    "    numSum+=num\n",
    "\n",
    "    \n",
    "    #Now lets summarize everything showing the sum of values \n",
    "    values.append([\"sum\",str(round(posSum,2)),str(round(negSum,2)),str(round(dollSum,2)),str(round(covSum/numSum,2)),(str(round(posSum/covSum,2)) if (covSum>0) else \"\")])\n",
    "    \n",
    "    return values,columns\n",
    "    \n",
    "    \n",
    "################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startCNN = datetime.datetime.now()\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from math import floor\n",
    "#from ensemble import ensemble\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "#sys.argv[1] --input1\n",
    "#sys.argv[2] --input2\n",
    "#sys.argv[3] --input3\n",
    "#sys.argv[4] --input4\n",
    "input1='teste-adam-0.3-relu'\n",
    "input2='results-adam-relu-explorations-0.3'\n",
    "input3='1'\n",
    "input4='0'\n",
    "\n",
    "outputFile=str(input2)+\".pdf\"\n",
    "numFiles=int(input3)\n",
    "#Number of epochs in the algorithm\n",
    "numEpochs=35\n",
    "numPlots=10\n",
    "\n",
    "pdf=PdfPages(outputFile)\n",
    "\n",
    "#Configure the size of the picture that will be plotted\n",
    "#Configure the size of the picture that will be plotted\n",
    "plt.figure(figsize=((numEpochs/10)*(2),9*5))\n",
    "\n",
    "#Open the file that was saved on folder csv/walks, containing information about each iteration in that walk \n",
    "#Lets show a summary of each walk\n",
    "#For each walk, one column is plotted in a final pdf file\n",
    "\n",
    "doc = pd.read_csv(\"Output/csv/distCNNs/distCNNs-agent-training.csv\")\n",
    "\n",
    "a_test=(doc.loc[:, 'testAccuracy'])[1]\n",
    "print('Test acuracy: ',a_test)\n",
    "a_train=(doc.loc[:, 'trainAccuracy'])[1]\n",
    "print('Train acuracy: ',a_train)\n",
    "a_val=(doc.loc[:, 'validationAccuracy'])[1]\n",
    "print('Validation acuracy: ',a_val)\n",
    "      \n",
    "a_test=(doc.loc[:, 'testLongAcc'])[1]\n",
    "print('Test LongAcc: ',a_test)\n",
    "a_train=(doc.loc[:, 'trainLongAcc'])[1]\n",
    "print('Train LongAcc: ',a_train)\n",
    "a_val=(doc.loc[:, 'validationLongAcc'])[1]\n",
    "print('Validation LongAcc: ',a_val)\n",
    "      \n",
    "a_test=(doc.loc[:, 'testShortAcc'])[1]\n",
    "print('Test ShortAcc: ',a_test)\n",
    "a_train=(doc.loc[:, 'trainShortAcc'])[1]\n",
    "print('Train ShortAcc: ',a_train)\n",
    "a_val=(doc.loc[:, 'validationShortAcc'])[1]\n",
    "print('Validation ShortAcc: ',a_val)\n",
    "      \n",
    "a_test=(doc.loc[:, 'testCoverage'])[1]\n",
    "print('Test Coverage: ',a_test)\n",
    "a_train=(doc.loc[:, 'trainCoverage'])[1]\n",
    "print('Train Coverage: ',a_train)\n",
    "a_val=(doc.loc[:, 'validationCoverage'])[1]\n",
    "print('Validation Coverage: ',a_val)\n",
    "      \n",
    "a_test=(doc.loc[:, 'testReward'])[1]\n",
    "print('Test Reward: ',a_test)\n",
    "a_train=(doc.loc[:, 'trainReward'])[1]\n",
    "print('Train Reward: ',a_train)\n",
    "a_val=(doc.loc[:, 'validationReward'])[1]\n",
    "print('Validation Reward: ',a_val)\n",
    "\n",
    "for i in range(1,numFiles+1):\n",
    "\n",
    "    document = pd.read_csv(\"Output/csv/distCNNs/distCNNs-agent-training.csv\")\n",
    "    plt.subplot(numPlots,numFiles,0*numFiles + i)\n",
    "    #Draw information in that file. First of all, lets plot accuracy\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testAccuracy'].tolist(),'r',label='Test')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainAccuracy'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationAccuracy'].tolist(),'g',label='Validation')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    #Lets draw information about coverage, read from the csv file located at csv/walks\n",
    "    plt.subplot(numPlots,numFiles,1*numFiles + i)\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testCoverage'].tolist(),'r',label='Test')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainCoverage'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationCoverage'].tolist(),'g',label='Validation')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Coverage')\n",
    "\n",
    "    # Information about reward\n",
    "    plt.subplot(numPlots,numFiles,2*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainReward'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationReward'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testReward'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Reward')\n",
    "    \n",
    "    #Percentages of long\n",
    "    plt.subplot(numPlots,numFiles,3*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainLong%'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationLong%'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testLong%'].tolist(),'r',label='Test')  \n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))    \n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Long %')\n",
    "    \n",
    "    #Percentages of short\n",
    "    plt.subplot(numPlots,numFiles,4*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainShort%'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationShort%'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testShort%'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Short %')\n",
    "    \n",
    "\n",
    "    #Coverage\n",
    "    plt.subplot(numPlots,numFiles,5*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),list(map(lambda x: 1-x,document.loc[:, 'trainCoverage'].tolist())),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),list(map(lambda x: 1-x,document.loc[:, 'validationCoverage'].tolist())),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),list(map(lambda x: 1-x,document.loc[:, 'testCoverage'].tolist())),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Hold %')\n",
    "    \n",
    "\n",
    "    #Accuracy of longs\n",
    "    plt.subplot(numPlots,numFiles,6*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainLongAcc'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationLongAcc'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testLongAcc'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Long Accuracy')\n",
    "    \n",
    "    #Accuracy of shorts\n",
    "    plt.subplot(numPlots,numFiles,7*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainShortAcc'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validationShortAcc'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testShortAcc'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Short Accuracy')\n",
    "\n",
    "    \n",
    "    #Precisions of long\n",
    "    plt.subplot(numPlots,numFiles,8*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainLongPrec'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validLongPrec'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testLongPrec'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Long Precision')\n",
    "    \n",
    "    #Precisions of short\n",
    "    plt.subplot(numPlots,numFiles,9*numFiles + i )\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'trainShortPrec'].tolist(),'b',label='Train')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'validShortPrec'].tolist(),'g',label='Validation')\n",
    "    plt.plot(document.loc[:, 'Iteration'].tolist(),document.loc[:, 'testShortPrec'].tolist(),'r',label='Test')\n",
    "    plt.xticks(range(0,numEpochs,4))\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Short Precision')\n",
    "\n",
    "pt = datetime.datetime.now()-startCNN\n",
    "total_seconds = pt.total_seconds()\n",
    "print('Step 3. Majority voting stage execution_time (s): ',total_seconds)\n",
    "\n",
    "#plt.suptitle(\"Experiment RL metalearner 35 neurons single layer\",size=19,weight=20,ha='left',x=0.1,y=0.99)\n",
    "\n",
    "pdf.savefig()\n",
    "\n",
    "\n",
    "#Now, lets try the ensemble\n",
    "i=1\n",
    "\n",
    "###########-------------------------------------------------------------------|Tabella Full Ensemble|-------------------\n",
    "x=2\n",
    "y=1\n",
    "plt.figure(figsize=(x*3.5,y*3.5))\n",
    "\n",
    "plt.subplot(y,y,1)\n",
    "plt.axis('off')\n",
    "val,col=ensemble(\"test\", input1)\n",
    "t=plt.table(cellText=val, colLabels=col, fontsize=20, loc='center')\n",
    "t.auto_set_font_size(False)\n",
    "t.set_fontsize(6)\n",
    "plt.title(\"Final Results\")\n",
    "#plt.suptitle(\"MAJORITY VOTING\")\n",
    "pdf.savefig()\n",
    "###########--------------------------------------------------------------------------------------------------------------------\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
